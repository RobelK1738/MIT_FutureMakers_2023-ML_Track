# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
* Activation functions and their properties

## Challenging, interesting, or exciting aspects of today's assignment
* Trajectories and graphs of activation functions

## Additional resources used 
* https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
* ChatGPT
